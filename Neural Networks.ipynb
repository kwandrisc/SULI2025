{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "362ee45e-e40d-4bb4-a453-834402aa229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from typing import List, Dict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import uproot\n",
    "import os\n",
    "import glob\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74a78ad1-ef73-4800-88b9-96a486115584",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'(' was never closed (242134116.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 15\u001b[0;36m\u001b[0m\n\u001b[0;31m    model = mlpClassifier(window\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def mlpClassifier(input_df, outputs):\n",
    "    input_hits = tf.keras.Input(input_df)\n",
    "\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(input_hits)\n",
    "    x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
    "    output = tf.keras.layers.Dense(n_outputFeatures, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_hits], outputs=[output])\n",
    "    return model\n",
    "\n",
    "model = mlpClassifier(window\n",
    "\n",
    "Model.fit(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f7b1975-2c9b-4791-8b70-8784aabff92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlpScikitClassifier:\n",
    "    def __init__(self, yaml_file: str): # identifying that the yaml file should be a string\n",
    "        \"\"\"\n",
    "        Initialize with configurations from a YAML file.\n",
    "        :param yaml_file: Path to the YAML configuration file.\n",
    "        \"\"\"\n",
    "        # defining each of the attributes\n",
    "        self.input_paths = None\n",
    "        self.samples = {}\n",
    "        self.variables = []\n",
    "        self.hyperparameters = {}\n",
    "        self.model = None\n",
    "        self.dataframes = {\"background\": None, \"signal\": None}\n",
    "\n",
    "        # calling two of the functions up here, even though only defined below\n",
    "        # can do this bc reads classes all first, then goes through\n",
    "        self._parse_yaml(yaml_file)\n",
    "        self._load_data()\n",
    "\n",
    "    def _parse_yaml(self, yaml_file: str):\n",
    "        try: # try means that if error happens, it'll go to exception\n",
    "            # use 'with' to make sure that yaml file is closed after it's opened\n",
    "            with open(yaml_file, 'r' ) as file: # telling to read the file and call it \"file\"\n",
    "                config = yaml.safe_load(file)\n",
    "\n",
    "            # defining the attributes from things inside yaml file\n",
    "            self.input_paths = config['input_path']\n",
    "            self.samples[0] = config.get(0, []) # defining first column of samples as those under the key 0 in the yaml file - also, if 0 doesn't exist, return empty list \"[]\"\n",
    "            self.samples[1] = config.get(1, []) # defining second column of samples to be things under key 1 in file\n",
    "            self.variables = config.get('variables', []) # defining variables to be things under key 'variables'\n",
    "\n",
    "        # if error occurs inside the try, will give this formatted error\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error parsing YAML file: {e}\") # f-string that lets you embed variables in string\n",
    "\n",
    "    def _load_data(self):\n",
    "        for sample_type, key in [(\"background\", 0), (\"signal\", 1)]: # iterating this for loop, once for signal, once for background\n",
    "            sample_list = self.samples[key] # calling all the background/signal samples\n",
    "            dataframes = []\n",
    "\n",
    "            for sample in sample_list:\n",
    "                files = glob.glob(os.path.join(f\"{self.input_paths}/{sample}/\", \"*.root\")) # joining the sample with input path and making root file\n",
    "                for file in files: ## why do we need this for loop, if there should only be one file found for each sample?\n",
    "                    self.tree = uproot.open(file)['analysis'] # opening root file and ttree\n",
    "                    variables_with_b_w = self.variables + ['b_w']\n",
    "                    df = self.tree.arrays(variables_with_b_w, library=\"pd\") # turning tree into pd df\n",
    "                    df['sample'] = sample # adding column for sample name\n",
    "                    df['label'] = key # adding column for 0 or 1\n",
    "                    dataframes.append(df) # adding all background df into one list\n",
    "\n",
    "            combined_df = pd.concat(dataframes, ignore_index=True).dropna() # concatinating the list of df into one full df\n",
    "            self.dataframes[sample_type] = combined_df # making the df an attribute, and placing them in dictionary with the key of either signal or background\n",
    "\n",
    "    def print_config(self):\n",
    "        \"\"\"Prints the current configuration.\"\"\"\n",
    "        print(f\"Input Paths: {self.input_paths}\")\n",
    "        print(f\"Samples: {self.samples}\")\n",
    "        print(f\"Variables: {self.variables}\")\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # putting data into sep background and signal df\n",
    "        self.background_df = self.dataframes[\"background\"]\n",
    "        self.signal_df = self.dataframes[\"signal\"]\n",
    "\n",
    "        # computing ratio between sig and back for reweighting when training model\n",
    "        #self.scale = len(background_df) / len(signal_df)\n",
    "\n",
    "        self.combined_df = pd.concat([self.background_df, self.signal_df], ignore_index=True) # combining sig and back df into one df\n",
    "        # splitting df into X and y\n",
    "        self.X = self.combined_df[self.variables]  # Use only the specified variables\n",
    "        self.y = self.combined_df['label']\n",
    "        self.weights = self.combined_df['b_w']\n",
    "\n",
    "        # returning the train/test split so can be used later in class\n",
    "        return train_test_split(self.X, self.y, self.weights, test_size=0.3, random_state=42)\n",
    "\n",
    "    def train_model(self, model_path=\"xgboost_model.json\", importance_plot_path=None): # function parameters with defaults (if don't pass anything when calling function)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test, b_w_train, b_w_test = self.prepare_data() # defining X and y train/test\n",
    "\n",
    "        def create_baseline():\n",
    "            model = Sequential()\n",
    "            inputShape = len(self.variables)\n",
    "            model.add(Dense(64, input_shape=(inputShape,), activation='relu'))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "            return model\n",
    "\n",
    "        estimator = KerasClassifier(model=create_baseline, epochs=50, batch_size=10, verbose=1)\n",
    "        kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "        results = cross_val_score(estimator, self.X_train, self.y_train, cv=kfold)\n",
    "        print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af3710e-3fc5-4b15-a956-c884ba3ce424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/k/kwandr/.local/perlmutter/python-3.11/lib/python3.11/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "W0000 00:00:1752783482.981055  461155 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m 433402/1525860\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12:24\u001b[0m 681us/step - accuracy: 0.9477 - loss: 2.8239"
     ]
    }
   ],
   "source": [
    "mlp = mlpScikitClassifier('nersc.yaml')\n",
    "mlp.train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
